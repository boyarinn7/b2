import json
import os
import sys
import requests
import openai
import textstat
import spacy
import re

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ modules
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..', 'modules')))

from datetime import datetime
from logger import get_logger
from error_handler import handle_error
from utils import ensure_directory_exists
from config_manager import ConfigManager

# === –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ===
logger = get_logger("generate_content")
config = ConfigManager()

# === –ù–∞—Å—Ç—Ä–æ–π–∫–∞ OpenAI API ===
try:
    openai.api_key = config.get('API_KEYS.openai.api_key')
    openai_model = config.get('API_KEYS.openai.model', 'gpt-4')
    if not openai.api_key:
        raise ValueError("‚ùå API –∫–ª—é—á OpenAI –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏.")
    logger.info("‚úÖ OpenAI API –∫–ª—é—á —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω.")
    logger.info(f"üîß –ò—Å–ø–æ–ª—å–∑—É–µ–º–∞—è –º–æ–¥–µ–ª—å OpenAI: {openai_model}")
except Exception as e:
    handle_error("OpenAI Initialization Error", str(e))


class ContentGenerator:
    def __init__(self):
        self.topic_threshold = config.get('GENERATE.topic_threshold', 7)
        self.text_threshold = config.get('GENERATE.text_threshold', 8)
        self.max_attempts = config.get('GENERATE.max_attempts', 3)
        self.adaptation_enabled = config.get('GENERATE.adaptation_enabled', False)
        self.adaptation_params = config.get('GENERATE.adaptation_parameters', {})
        self.content_output_path = config.get('FILE_PATHS.content_output_path', 'core/generated_content.json')
        self.before_critique_path = config.get('FILE_PATHS.before_critique_path', 'core/before_critique.json')
        self.after_critique_path = config.get('FILE_PATHS.after_critique_path', 'core/after_critique.json')

        self.openai_model = config.get('API_KEYS.openai.model', 'gpt-4')
        self.temperature = config.get('API_KEYS.openai.temperature', 0.7)  # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã

    def analyze_seo(self, content):
        """
        SEO-–∞–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ Ubersuggest –∏–ª–∏ SEMrush API.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç SEO-–º–µ—Ç—Ä–∏–∫–∏, —Ç–∞–∫–∏–µ –∫–∞–∫ –ø–ª–æ—Ç–Ω–æ—Å—Ç—å –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏.
        """
        try:
            logger.info("üîç –í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è SEO-–∞–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞...")
            provider = config.get('EXTERNAL_TOOLS.seo_analysis.provider', 'ubersuggest')
            api_key = config.get('EXTERNAL_TOOLS.seo_analysis.api_key')

            if not api_key:
                raise ValueError("API-–∫–ª—é—á –¥–ª—è SEO-–∞–Ω–∞–ª–∏–∑–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –∫–æ–Ω—Ñ–∏–≥–µ.")

            if provider == 'ubersuggest':
                api_url = "https://app.neilpatel.com/ubersuggest/api"
                payload = {
                    "query": content,
                    "token": api_key,
                    "lang": "ru"
                }
                response = requests.post(api_url, json=payload)
            elif provider == 'semrush':
                api_url = "https://api.semrush.com/"
                payload = {
                    "type": "phrase_organic",
                    "key": api_key,
                    "phrase": content,
                    "database": "ru"
                }
                response = requests.get(api_url, params=payload)
            else:
                raise ValueError("–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä SEO-–∞–Ω–∞–ª–∏–∑–∞.")

            response.raise_for_status()
            seo_results = response.json()
            logger.info(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç SEO-–∞–Ω–∞–ª–∏–∑–∞ ({provider}): {seo_results}")
            return seo_results

        except requests.exceptions.HTTPError as http_err:
            handle_error("SEO Analysis HTTP Error", f"HTTP error occurred: {http_err}")
        except requests.exceptions.ConnectionError as conn_err:
            handle_error("SEO Analysis Connection Error", f"Connection error occurred: {conn_err}")
        except requests.exceptions.Timeout as timeout_err:
            handle_error("SEO Analysis Timeout Error", f"Timeout error occurred: {timeout_err}")
        except ValueError as ve:
            handle_error("SEO Analysis Value Error", str(ve))
        except Exception as e:
            handle_error("SEO Analysis Error", str(e))
        return {}

    def analyze_grammar(self, content):
        """
        –ü—Ä–æ–≤–µ—Ä–∫–∞ –≥—Ä–∞–º–º–∞—Ç–∏–∫–∏ –∏ —Å—Ç–∏–ª—è —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ LanguageTool API.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –æ—à–∏–±–æ–∫ –∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π.
        """
        try:
            logger.info("üîç –í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –≥—Ä–∞–º–º–∞—Ç–∏–∫–∏ –∏ —Å—Ç–∏–ª—è —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ LanguageTool API...")
            api_key = config.get('EXTERNAL_TOOLS.grammar_check.api_key')
            api_url = "https://api.languagetool.org/v2/check"

            payload = {
                "text": content,
                "language": "ru",
                "apiKey": api_key
            }
            response = requests.post(api_url, data=payload)
            response.raise_for_status()

            grammar_results = response.json()
            logger.info(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏ –≥—Ä–∞–º–º–∞—Ç–∏–∫–∏: {grammar_results}")
            return grammar_results

        except requests.exceptions.RequestException as e:
            handle_error("Grammar Analysis Error", str(e))
        except Exception as e:
            handle_error("Grammar Analysis Error", str(e))
        return {}

    def generate_sarcastic_comment(self, text):
        if not config.get('SARCASM.enabled', False):
            logger.info("üîï –°–∞—Ä–∫–∞–∑–º –æ—Ç–∫–ª—é—á—ë–Ω –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏.")
            return ""

        prompt = config.get('SARCASM.comment_prompt').format(text=text)
        try:
            response = openai.ChatCompletion.create(
                model=self.openai_model,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=config.get('CONTENT.sarcasm.max_tokens_comment', 100),
                temperature=self.temperature
            )
            comment = response['choices'][0]['message']['content'].strip()
            logger.info("üé© –°–∞—Ä–∫–∞—Å—Ç–∏—á–µ—Å–∫–∏–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —É—Å–ø–µ—à–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω.")
            return comment
        except Exception as e:
            handle_error("Sarcasm Comment Generation Error", str(e))
            return ""

    def generate_interactive_poll(self, text):
        if not config.get('SARCASM.enabled', False):
            logger.info("üîï –°–∞—Ä–∫–∞–∑–º –æ—Ç–∫–ª—é—á—ë–Ω –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏.")
            return ""

        prompt = config.get('SARCASM.question_prompt').format(text=text)
        try:
            response = openai.ChatCompletion.create(
                model=self.openai_model,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=config.get('CONTENT.sarcasm.max_tokens_poll', 150),
                temperature=self.temperature
            )
            poll = response['choices'][0]['message']['content'].strip()
            logger.info("üé≠ –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –≤–æ–ø—Ä–æ—Å —É—Å–ø–µ—à–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω.")
            return poll
        except Exception as e:
            handle_error("Sarcasm Poll Generation Error", str(e))
            return ""

    def append_sarcasm_to_post(self, text):
        """
        –î–æ–±–∞–≤–ª—è–µ—Ç —Å–∞—Ä–∫–∞—Å—Ç–∏—á–µ—Å–∫–∏–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –∏ –æ–ø—Ä–æ—Å –∫ –æ—Å–Ω–æ–≤–Ω–æ–º—É —Ç–µ–∫—Å—Ç—É, —Å —á–µ—Ç–∫–∏–º —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º –±–ª–æ–∫–æ–≤.
        """
        if not config.get('SARCASM.enabled', False):
            logger.info("üîï –°–∞—Ä–∫–∞–∑–º –æ—Ç–∫–ª—é—á—ë–Ω –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏.")
            return text

        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∞—Ä–∫–∞—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è
        comment = self.generate_sarcastic_comment(text)
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞
        poll = self.generate_interactive_poll(text)

        # –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç –Ω–µ–Ω—É–∂–Ω—ã—Ö —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–π –∏ –≤—Å—Ç–∞–≤–æ–∫
        cleaned_text = text.replace("**–¢–µ–∫—Å—Ç:**", "").replace("‚ú® –ò–Ω—Ç–µ—Ä–µ—Å–Ω—ã–π —Ñ–∞–∫—Ç:", "").replace("üéØ –í–∞–∂–Ω–æ:", "")
        cleaned_text = re.sub(r"\*\*.*?\*\*", "", cleaned_text).strip()

        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —Å —á–µ—Ç–∫–∏–º–∏ –º–µ—Ç–∫–∞–º–∏
        final_text = (
            "üî∑üî∑üî∑ **–û–°–ù–û–í–ù–û–ô –¢–ï–ö–°–¢** üî∑üî∑üî∑\n"
            f"{cleaned_text.strip()}"
        )

        if comment:
            final_text += (
                "\n\nüî∂üî∂üî∂ **–°–ê–†–ö–ê–°–¢–ò–ß–ï–°–ö–ò–ô –ö–û–ú–ú–ï–ù–¢–ê–†–ò–ô** üî∂üî∂üî∂\n"
                f"{comment.strip()}"
            )
        if poll:
            final_text += (
                "\n\nüî∏üî∏üî∏ **–ò–ù–¢–ï–†–ê–ö–¢–ò–í–ù–´–ô –í–û–ü–†–û–°** üî∏üî∏üî∏\n"
                f"{poll.strip()}"
            )

        logger.info("‚úÖ –ë–∞—Ä–æ–Ω –°–∞—Ä–∫–∞–∑–º –¥–æ–±–∞–≤–∏–ª —Å–≤–æ–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –∏ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –æ–ø—Ä–æ—Å.")
        return final_text

    def analyze_topic_generation(self):
        """
        –ê–Ω–∞–ª–∏–∑ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏, –∞—Ä—Ö–∏–≤–∞ –∏ —Ñ–æ–∫—É—Å–æ–≤.
        """
        try:
            logger.info("üîç –ê–Ω–∞–ª–∏–∑ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–º: –æ–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å, –∞—Ä—Ö–∏–≤ –∏ –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Ñ–æ–∫—É—Å—ã...")

            # === 1. –ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ ===
            feedback_path = config.get('FILE_PATHS.feedback_file', 'core/data/feedback.json')
            if os.path.exists(feedback_path):
                with open(feedback_path, 'r', encoding='utf-8') as file:
                    feedback_data = json.load(file)
                positive_feedback_topics = [entry['topic'] for entry in feedback_data if entry.get('rating', 0) >= 8]
                logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –æ–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å: {len(positive_feedback_topics)} —É—Å–ø–µ—à–Ω—ã—Ö —Ç–µ–º.")
            else:
                positive_feedback_topics = []
                logger.warning("‚ö†Ô∏è –§–∞–π–ª –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω.")

            # === 2. –ê–Ω–∞–ª–∏–∑ –∞—Ä—Ö–∏–≤–∞ —É—Å–ø–µ—à–Ω—ã—Ö –ø—É–±–ª–∏–∫–∞—Ü–∏–π ===
            archive_folder = config.get('FILE_PATHS.archive_folder', 'core/archive/')
            successful_topics = []
            if os.path.exists(archive_folder):
                for filename in os.listdir(archive_folder):
                    if filename.endswith('.json'):
                        with open(os.path.join(archive_folder, filename), 'r', encoding='utf-8') as file:
                            archive_data = json.load(file)
                        if archive_data.get('success', False):
                            successful_topics.append(archive_data.get('topic', ''))
                logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∏–∑ –∞—Ä—Ö–∏–≤–∞: {len(successful_topics)} —É—Å–ø–µ—à–Ω—ã—Ö —Ç–µ–º.")
            else:
                logger.warning("‚ö†Ô∏è –ü–∞–ø–∫–∞ –∞—Ä—Ö–∏–≤–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞.")

            # === 3. –ò—Å–∫–ª—é—á–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö 10 —Ñ–æ–∫—É—Å–æ–≤ ===
            last_focus_areas = config.get('CONTENT.topic.focus_areas', [])[-10:]
            logger.info(f"üîÑ –ü–æ—Å–ª–µ–¥–Ω–∏–µ 10 —Ñ–æ–∫—É—Å–æ–≤: {last_focus_areas}")

            # === 4. –ö–æ–º–ø–∏–ª—è—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ ===
            combined_focus_areas = list(set(positive_feedback_topics + successful_topics + last_focus_areas))
            if not combined_focus_areas:
                combined_focus_areas = config.get('CONTENT.topic.focus_areas', [])

            logger.info(f"üìä –§–∏–Ω–∞–ª—å–Ω—ã–µ —Ñ–æ–∫—É—Å—ã –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {combined_focus_areas}")

            return combined_focus_areas

        except Exception as e:
            handle_error("Topic Analysis Error", str(e))
            return config.get('CONTENT.topic.focus_areas', [])

    def adapt_prompts(self):
        """–ê–¥–∞–ø—Ç–∞—Ü–∏—è –ø—Ä–æ–º–ø—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏–∑ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏."""
        if not self.adaptation_enabled:
            logger.info("üîÑ –ê–¥–∞–ø—Ç–∞—Ü–∏—è –ø—Ä–æ–º–ø—Ç–æ–≤ –æ—Ç–∫–ª—é—á–µ–Ω–∞.")
            return

        logger.info("üîÑ –ü—Ä–∏–º–µ–Ω—è—é –∞–¥–∞–ø—Ç–∞—Ü–∏—é –ø—Ä–æ–º–ø—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏...")
        for key, value in self.adaptation_params.items():
            logger.info(f"üîß –ü–∞—Ä–∞–º–µ—Ç—Ä '{key}' –æ–±–Ω–æ–≤–ª—ë–Ω –¥–æ {value}")

    def generate_topic(self):
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–º—ã —Å —É—á—ë—Ç–æ–º –∏—Å–∫–ª—é—á–µ–Ω–∏–π, –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –∏ –∞—Ä—Ö–∏–≤–∞.
        """
        try:
            # –ü–æ–ª—É—á–∞–µ–º –¥–æ–ø—É—Å—Ç–∏–º—ã–µ —Ñ–æ–∫—É—Å—ã
            valid_focus_areas = self.get_valid_focus_areas()

            # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –∏ –∞—Ä—Ö–∏–≤–∞
            chosen_focus = self.prioritize_focus_from_feedback_and_archive(valid_focus_areas)

            if not chosen_focus:
                raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–±—Ä–∞—Ç—å —Ñ–æ–∫—É—Å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–º—ã.")

            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–º—ã
            prompt_template = config.get('CONTENT.topic.prompt_template')
            prompt = prompt_template.format(focus_areas=chosen_focus)

            logger.info("üîÑ –ó–∞–ø—Ä–æ—Å –∫ OpenAI –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–º—ã...")
            topic = self.request_openai(prompt)

            self.update_focus_tracker(chosen_focus)
            self.save_to_generated_content("topic", {"topic": topic})

            logger.info(f"‚úÖ –¢–µ–º–∞ —É—Å–ø–µ—à–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–∞: {topic}")
            return topic

        except Exception as e:
            handle_error("Topic Generation Error", str(e))

    def clear_generated_content(self):
        """
        –ü–æ–ª–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ —Ñ–∞–π–ª–∞ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –ø–µ—Ä–µ–¥ –∑–∞–ø–∏—Å—å—é –Ω–æ–≤–æ–π —Ç–µ–º—ã.
        """
        try:
            logger.info("üßπ –ü–æ–ª–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ —Ñ–∞–π–ª–∞ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –ø–µ—Ä–µ–¥ –∑–∞–ø–∏—Å—å—é –Ω–æ–≤–æ–π —Ç–µ–º—ã.")

            # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –ø–∞–ø–∫–∞ –¥–ª—è —Ñ–∞–π–ª–∞ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
            folder = os.path.dirname(self.content_output_path)
            if not os.path.exists(folder):
                os.makedirs(folder)
                logger.info(f"üìÅ –ü–∞–ø–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö —Å–æ–∑–¥–∞–Ω–∞: {folder}")

            # –ü–æ–ª–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ —Ñ–∞–π–ª–∞
            with open(self.content_output_path, 'w', encoding='utf-8') as file:
                json.dump({}, file, ensure_ascii=False, indent=4)

            logger.info("‚úÖ –§–∞–π–ª —É—Å–ø–µ—à–Ω–æ –æ—á–∏—â–µ–Ω.")
        except PermissionError:
            handle_error("Clear Content Error", f"–ù–µ—Ç –ø—Ä–∞–≤ –Ω–∞ –∑–∞–ø–∏—Å—å –≤ —Ñ–∞–π–ª: {self.content_output_path}")
        except Exception as e:
            handle_error("Clear Content Error", str(e))

    def request_openai(self, prompt):
        """–ó–∞–ø—Ä–æ—Å –∫ OpenAI –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞."""
        try:
            response = openai.ChatCompletion.create(
                model=self.openai_model,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=config.get('CONTENT.text.max_tokens', 750),
                temperature=config.get('API_KEYS.openai.temperature', 0.7)
            )
            return response['choices'][0]['message']['content'].strip()
        except Exception as e:
            handle_error("OpenAI API Error", e)

    def critique_content(self, content):
        """–ö—Ä–∏—Ç–∏–∫–∞ —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ OpenAI API."""
        try:
            logger.info("üîÑ –ó–∞–ø—Ä–æ—Å –∫ OpenAI –¥–ª—è –∫—Ä–∏—Ç–∏–∫–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞...")
            critique = self.request_openai(f"–ü—Ä–æ–≤–µ–¥–∏ –∞–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞:\n{content}")
            logger.info("‚úÖ –ö—Ä–∏—Ç–∏–∫–∞ —É—Å–ø–µ—à–Ω–æ –ø—Ä–æ–≤–µ–¥–µ–Ω–∞.")
            self.save_to_generated_content("critique", {"critique": critique})
            return critique
        except Exception as e:
            handle_error("Critique Error", e)

    def improve_content(self, content, critique, readability_results=None, semantic_results=None):
        """
        –£–ª—É—á—à–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —Å —É—á—ë—Ç–æ–º –∫—Ä–∏—Ç–∏–∫–∏ –∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∞–Ω–∞–ª–∏–∑–∞.
        """


        try:
            logger.info("üîÑ –ó–∞–ø—Ä–æ—Å –∫ OpenAI –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞...")

            # –ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö, –µ—Å–ª–∏ –æ–Ω–∏ –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω—ã
            readability_results = readability_results or {}
            semantic_results = semantic_results or {}

            prompt_template = config.get('CONTENT.improve.prompt_template')
            prompt = prompt_template.format(
                critique=critique,
                readability_results=readability_results,
                semantic_results=semantic_results,
                content=content
            )

            response = openai.ChatCompletion.create(
                model=self.openai_model,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=config.get('CONTENT.improve.max_tokens', 600),
                temperature=self.temperature
            )

            text_after_critique = response['choices'][0]['message']['content'].strip()

            # –£–±–∏—Ä–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ —Ñ—Ä–∞–∑—ã –∏–∑ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
            text_after_critique = text_after_critique.replace(
                "üéØ –í–∞–∂–Ω–æ: –ü–æ—Å—Ç–æ—è–Ω–Ω–æ –¥–µ—Ä–∂–∏—Ç–µ –≤ —É–º–µ –æ—Å–Ω–æ–≤–Ω—É—é —Ç–µ–º—É. –ù–µ –æ—Ç–∫–ª–æ–Ω—è–π—Ç–µ—Å—å –æ—Ç –≥–ª–∞–≤–Ω–æ–π –∏–¥–µ–∏ —Ç–µ–∫—Å—Ç–∞.",
                "")

            improved_content = response['choices'][0]['message']['content'].strip()
            logger.info("‚úÖ –¢–µ–∫—Å—Ç —É—Å–ø–µ—à–Ω–æ —É–ª—É—á—à–µ–Ω.")
            return improved_content

        except Exception as e:
            handle_error("Improvement Error", e)
            return content

    def save_to_generated_content(self, stage, data):
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –Ω–∞ –∫–∞–∂–¥–æ–º —ç—Ç–∞–ø–µ, –æ–±–Ω–æ–≤–ª—è—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ –≤ —Ñ–∞–π–ª–µ.

        –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
            stage (str): –≠—Ç–∞–ø —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'topic', 'critique', 'text_initial').
            data (dict): –î–∞–Ω–Ω—ã–µ, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å.
        """
        try:
            logger.info(f"üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —Ñ–∞–π–ª: {self.content_output_path}")

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è –ø–∞–ø–∫–∏ –∏ –µ—ë —Å–æ–∑–¥–∞–Ω–∏–µ, –µ—Å–ª–∏ –æ–Ω–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç
            folder = os.path.dirname(self.content_output_path)
            if not os.path.exists(folder):
                os.makedirs(folder)
                logger.info(f"üìÅ –ü–∞–ø–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö —Å–æ–∑–¥–∞–Ω–∞: {folder}")

            # –ß—Ç–µ–Ω–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
            if os.path.exists(self.content_output_path):
                with open(self.content_output_path, 'r', encoding='utf-8') as file:
                    try:
                        result_data = json.load(file)
                    except json.JSONDecodeError:
                        result_data = {}
            else:
                result_data = {}

            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ —ç—Ç–∞–ø–∞
            result_data["timestamp"] = datetime.utcnow().isoformat()
            result_data[stage] = data

            # –ó–∞–ø–∏—Å—å –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –æ–±—Ä–∞—Ç–Ω–æ –≤ —Ñ–∞–π–ª
            with open(self.content_output_path, 'w', encoding='utf-8') as file:
                json.dump(result_data, file, ensure_ascii=False, indent=4)

            logger.info(f"‚úÖ –î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω—ã –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –Ω–∞ —ç—Ç–∞–ø–µ: {stage}")

        except FileNotFoundError:
            handle_error("Save to Generated Content Error", f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {self.content_output_path}")
        except PermissionError:
            handle_error("Save to Generated Content Error", f"–ù–µ—Ç –ø—Ä–∞–≤ –Ω–∞ –∑–∞–ø–∏—Å—å –≤ —Ñ–∞–π–ª: {self.content_output_path}")
        except Exception as e:
            handle_error("Save to Generated Content Error", str(e))


    def analyze_readability(self, content):
            """–ê–Ω–∞–ª–∏–∑ —á–∏—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é TextStat."""
            try:
                logger.info("üîç –í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –∞–Ω–∞–ª–∏–∑ —á–∏—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞...")
                readability_score = textstat.flesch_reading_ease(content)
                word_count = textstat.lexicon_count(content)
                sentence_count = textstat.sentence_count(content)

                readability_results = {
                    "readability_score": readability_score,
                    "word_count": word_count,
                    "sentence_count": sentence_count
                }

                logger.info(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞ —á–∏—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç–∏: {readability_results}")
                return readability_results
            except Exception as e:
                handle_error("Readability Analysis Error", e)

    def update_focus_tracker(self, new_focus):
        """
        –î–æ–±–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π —Ñ–æ–∫—É—Å –≤ –Ω–∞—á–∞–ª–æ —Å–ø–∏—Å–∫–∞ –∏ —É–¥–∞–ª—è–µ—Ç —Å—Ç–∞—Ä—ã–µ, –µ—Å–ª–∏ –º–µ—Ç–æ–∫ –±–æ–ª—å—à–µ 200.
        """
        try:
            tracker_file = config.get('FILE_PATHS.focus_tracker', 'core/data/focus_tracker.json')

            # –°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∞–π–ª–∞, –µ—Å–ª–∏ –æ–Ω –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç
            if not os.path.exists(tracker_file):
                with open(tracker_file, 'w', encoding='utf-8') as file:
                    json.dump([], file)

            # –ß—Ç–µ–Ω–∏–µ —Ç–µ–∫—É—â–∏—Ö –º–µ—Ç–æ–∫
            with open(tracker_file, 'r', encoding='utf-8') as file:
                focus_tracker = json.load(file)

            # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—É—é –º–µ—Ç–∫—É –≤ –Ω–∞—á–∞–ª–æ
            focus_tracker.insert(0, new_focus)

            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Å–ø–∏—Å–æ–∫ 200 –º–µ—Ç–∫–∞–º–∏
            if len(focus_tracker) > 200:
                focus_tracker = focus_tracker[:200]

            # –ó–∞–ø–∏—Å—å –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫
            with open(tracker_file, 'w', encoding='utf-8') as file:
                json.dump(focus_tracker, file, ensure_ascii=False, indent=4)

            logger.info(f"‚úÖ –§–æ–∫—É—Å '{new_focus}' –¥–æ–±–∞–≤–ª–µ–Ω –≤ tracker. –í—Å–µ–≥–æ –º–µ—Ç–æ–∫: {len(focus_tracker)}.")

        except Exception as e:
            handle_error("Focus Tracker Update Error", str(e))

    def get_valid_focus_areas(self):
        """
        –ò—Å–∫–ª—é—á–∞–µ—Ç –ø–µ—Ä–≤—ã–µ 10 —Ñ–æ–∫—É—Å–æ–≤ –∏–∑ focus_tracker.json –∏–∑ —Å–ø–∏—Å–∫–∞ focus_areas.
        """
        try:
            tracker_file = config.get('FILE_PATHS.focus_tracker', 'core/data/focus_tracker.json')
            focus_areas = config.get('CONTENT.topic.focus_areas', [])

            # –ß—Ç–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Ñ–æ–∫—É—Å–æ–≤
            if os.path.exists(tracker_file):
                with open(tracker_file, 'r', encoding='utf-8') as file:
                    focus_tracker = json.load(file)
                excluded_foci = focus_tracker[:10]
            else:
                excluded_foci = []

            # –ò—Å–∫–ª—é—á–µ–Ω–∏–µ —Ñ–æ–∫—É—Å–æ–≤
            valid_focus_areas = [focus for focus in focus_areas if focus not in excluded_foci]

            logger.info(f"üîÑ –ò—Å–∫–ª—é—á–µ–Ω—ã 10 —Ñ–æ–∫—É—Å–æ–≤: {excluded_foci}")
            logger.info(f"‚úÖ –î–æ—Å—Ç—É–ø–Ω—ã–µ —Ñ–æ–∫—É—Å—ã –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {valid_focus_areas}")

            return valid_focus_areas

        except Exception as e:
            handle_error("Focus Area Filtering Error", str(e))
            return config.get('CONTENT.topic.focus_areas', [])

    def prioritize_focus_from_feedback_and_archive(self, valid_focus_areas):
        """
        –í—ã–±–∏—Ä–∞–µ—Ç –ø–µ—Ä–≤—ã–π –ø–æ–¥—Ö–æ–¥—è—â–∏–π —Ñ–æ–∫—É—Å –∏–∑ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –∏–ª–∏ –∞—Ä—Ö–∏–≤–∞.
        """
        try:
            # –û–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å
            feedback_path = config.get('FILE_PATHS.feedback_file', 'core/data/feedback.json')
            if os.path.exists(feedback_path):
                with open(feedback_path, 'r', encoding='utf-8') as file:
                    feedback_data = json.load(file)
                feedback_foci = [entry['topic'] for entry in feedback_data if entry.get('rating', 0) >= 8]
            else:
                feedback_foci = []

            # –ê—Ä—Ö–∏–≤
            archive_folder = config.get('FILE_PATHS.archive_folder', 'core/archive/')
            archive_foci = []
            if os.path.exists(archive_folder):
                for filename in os.listdir(archive_folder):
                    if filename.endswith('.json'):
                        with open(os.path.join(archive_folder, filename), 'r', encoding='utf-8') as file:
                            archive_data = json.load(file)
                        if archive_data.get('success', False):
                            archive_foci.append(archive_data.get('topic'))

            # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –≤—ã–±–æ—Ä–∞
            for focus in feedback_foci + archive_foci:
                if focus in valid_focus_areas:
                    logger.info(f"‚úÖ –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–π —Ñ–æ–∫—É—Å –≤—ã–±—Ä–∞–Ω –∏–∑ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –∏–ª–∏ –∞—Ä—Ö–∏–≤–∞: {focus}")
                    return focus

            # –ï—Å–ª–∏ –Ω–∏ –æ–¥–∏–Ω –Ω–µ –ø–æ–¥–æ—à—ë–ª, –≤—ã–±–∏—Ä–∞–µ–º –ø–µ—Ä–≤—ã–π –¥–æ—Å—Ç—É–ø–Ω—ã–π –∏–∑ valid_focus_areas
            if valid_focus_areas:
                logger.info(f"üîÑ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–µ—Ä–≤—ã–π –¥–æ—Å—Ç—É–ø–Ω—ã–π —Ñ–æ–∫—É—Å: {valid_focus_areas[0]}")
                return valid_focus_areas[0]

            logger.warning("‚ö†Ô∏è –ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Ñ–æ–∫—É—Å–æ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.")
            return None

        except Exception as e:
            handle_error("Focus Prioritization Error", str(e))
            return None



    def analyze_keywords(self, content):
            """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –∏ –≤—ã–¥–µ–ª–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ —Å –ø–æ–º–æ—â—å—é SpaCy."""
            try:
                logger.info("üîç –í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é SpaCy...")
                nlp = spacy.load("en_core_web_sm")
                doc = nlp(content)

                keywords = [token.text for token in doc if token.is_alpha and not token.is_stop]
                named_entities = [(ent.text, ent.label_) for ent in doc.ents]

                semantic_results = {
                    "keywords": keywords[:10],  # –ë–µ—Ä—ë–º —Ç–æ–ø-10 –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
                    "named_entities": named_entities
                }

                logger.info(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞: {semantic_results}")
                return semantic_results
            except Exception as e:
                handle_error("Semantic Analysis Error", e)

    def run(self):
        self.adapt_prompts()

        # –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –ø–µ—Ä–µ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –Ω–æ–≤–æ–π —Ç–µ–º—ã
        self.clear_generated_content()

        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–º—ã
        topic = self.generate_topic()
        self.save_to_generated_content("topic", {"topic": topic})

        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
        text_initial = self.request_openai(config.get('CONTENT.text.prompt_template').format(topic=topic))
        self.save_to_generated_content("text_initial", {"content": text_initial})

        # –ö—Ä–∏—Ç–∏–∫–∞ —Ç–µ–∫—Å—Ç–∞
        critique = self.request_openai(config.get('CONTENT.critique.prompt_template').format(content=text_initial))

        # –ê–Ω–∞–ª–∏–∑ —á–∏—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç–∏
        readability_results = self.analyze_readability(text_initial)
        self.save_to_generated_content("readability", readability_results)

        # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑
        semantic_results = self.analyze_keywords(text_initial)
        self.save_to_generated_content("semantic_analysis", semantic_results)

        # –£–ª—É—á—à–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
        text_after_critique = self.improve_content(
            text_initial, critique, readability_results, semantic_results
        )
        self.save_to_generated_content("text_improved", {"content": text_after_critique})

        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å–∞—Ä–∫–∞–∑–º–∞
        final_text = self.append_sarcasm_to_post(text_after_critique)
        self.save_to_generated_content("final_text", {"content": final_text})
        logger.info("üöÄ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")

if __name__ == "__main__":
    generator = ContentGenerator()
    generator.run()
