import os
import json
import logging
from botocore.exceptions import ClientError
from modules.api_clients import get_b2_client
from modules.logger import get_logger
from modules.error_handler import handle_error
from modules.utils import ensure_directory_exists
from modules.config_manager import ConfigManager
import subprocess  # –î–ª—è –∑–∞–ø—É—Å–∫–∞ –≤–Ω–µ—à–Ω–µ–≥–æ —Å–∫—Ä–∏–ø—Ç–∞
from scripts.generate_content import generate_file_id


# === –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è ===
config = ConfigManager()
logger = get_logger("b2_storage_manager")

# === –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ ===
B2_BUCKET_NAME = config.get('API_KEYS.b2.bucket_name')
CONFIG_PUBLIC_PATH = config.get('FILE_PATHS.config_public')
FILE_EXTENSIONS = ['.json', '.png', '.mp4']
FOLDERS = [
    config.get('FILE_PATHS.folder_444'),
    config.get('FILE_PATHS.folder_555'),
    config.get('FILE_PATHS.folder_666')
]
ARCHIVE_FOLDER = config.get('FILE_PATHS.archive_folder')

# –†–µ–≥—É–ª—è—Ä–Ω–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ñ–æ—Ä–º–∞—Ç–∞ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
import re
FILE_NAME_PATTERN = re.compile(r"^\d{8}-\d{4}\.\w+$")

def log_folders_state(s3, folders, stage):
    logger.info(f"\nüìÇ –°–æ—Å—Ç–æ—è–Ω–∏–µ –ø–∞–ø–æ–∫ ({stage}):")
    for folder in folders:
        files = list_files_in_folder(s3, folder)
        logger.info(f"- {folder}: {files}")

def load_config_public(s3):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç config_public.json –∏–∑ B2 –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—ã–µ generation_id."""
    try:
        local_path = os.path.basename(CONFIG_PUBLIC_PATH)
        s3.download_file(B2_BUCKET_NAME, CONFIG_PUBLIC_PATH, local_path)

        with open(local_path, 'r', encoding='utf-8') as file:
            config_data = json.load(file)

        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–ø–∏—Å–æ–∫ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—ã—Ö generation_id
        published_generations = config_data.get("published", [])

        return config_data, published_generations

    except FileNotFoundError:
        logger.error("‚ùå config_public.json –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ B2.")
        return {}, []

    except ClientError as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ config_public.json: {e.response['Error']['Message']}")
        return {}, []

def list_files_by_generation_id(s3, gen_id):
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ –≤ B2, —Å–æ–¥–µ—Ä–∂–∞—â–∏—Ö generation_id –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏."""
    try:
        all_files = list_files_in_bucket(s3)  # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤ –≤ B2
        matched_files = [f for f in all_files if gen_id in f]  # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ generation_id

        if not matched_files:
            logger.info(f"üîç –ù–∞–π–¥–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã –¥–ª—è {gen_id}: {matched_files}")
        return matched_files

    except ClientError as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ —Ñ–∞–π–ª–æ–≤ —Å generation_id {gen_id}: {e.response['Error']['Message']}")
        return []


def list_files_in_bucket(s3):
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤ –≤ B2."""
    try:
        response = s3.list_objects_v2(Bucket="boyarinnbotbucket")
        return [obj["Key"] for obj in response.get("Contents", [])]
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–ø–∏—Å–∫–∞ —Ñ–∞–π–ª–æ–≤ –≤ B2: {e}")
        return []


def archive_files(s3, files):
    """–ü–µ—Ä–µ–º–µ—â–∞–µ—Ç —Ñ–∞–π–ª—ã, —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ generation_id, –≤ data/archive/ –≤ B2."""
    try:
        logger.info(f"üì¶ –ê—Ä—Ö–∏–≤–∏—Ä—É–µ–º —Ñ–∞–π–ª—ã: {files}")

        for file in files:
            new_path = f"data/archive/{file.split('/')[-1]}"

            s3.copy_object(Bucket="boyarinnbotbucket",
                           CopySource={"Bucket": "boyarinnbotbucket", "Key": file},
                           Key=new_path)

            s3.delete_object(Bucket="boyarinnbotbucket", Key=file)

            logger.info(f"‚úÖ –§–∞–π–ª {file} –ø–µ—Ä–µ–º–µ—â—ë–Ω –≤ {new_path}.")

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞—Ä—Ö–∏–≤–∏—Ä–æ–≤–∞–Ω–∏–∏ —Ñ–∞–π–ª–æ–≤: {e}")


def save_config_public(s3, data):
    try:
        local_path = os.path.basename(CONFIG_PUBLIC_PATH)
        with open(local_path, 'w', encoding='utf-8') as file:
            json.dump(data, file, ensure_ascii=False, indent=4)
        s3.upload_file(local_path, B2_BUCKET_NAME, CONFIG_PUBLIC_PATH)
    except Exception as e:
        logger.error(f"Error saving config_public.json: {e}")

def cleanup_archive(s3, max_files=200):
    """–£–¥–∞–ª—è–µ—Ç —Å–∞–º—ã–µ —Å—Ç–∞—Ä—ã–µ —Ñ–∞–π–ª—ã –≤ data/archive/, –µ—Å–ª–∏ –∏—Ö –±–æ–ª—å—à–µ max_files.
       –§–∞–π–ª —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ–º .bzEmpty –Ω–µ —É–¥–∞–ª—è–µ—Ç—Å—è."""
    try:
        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤ –≤ –ø–∞–ø–∫–µ data/archive/
        all_files = list_files_in_folder(s3, "data/archive/")

        # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º .bzEmpty —Ñ–∞–π–ª—ã
        filtered_files = [f for f in all_files if not f.endswith(".bzEmpty")]

        # –ï—Å–ª–∏ —Ñ–∞–π–ª–æ–≤ <= max_files, –Ω–∏—á–µ–≥–æ –Ω–µ –¥–µ–ª–∞–µ–º
        if len(filtered_files) <= max_files:
            logger.info(f"‚úÖ –í –∞—Ä—Ö–∏–≤–µ {len(filtered_files)} —Ñ–∞–π–ª–æ–≤, –æ—á–∏—Å—Ç–∫–∞ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è.")
            return

        # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ñ–∞–π–ª–∞—Ö (–≤—Ä–µ–º—è —Å–æ–∑–¥–∞–Ω–∏—è)
        file_info = []
        for file in filtered_files:
            response = s3.head_object(Bucket="boyarinnbotbucket", Key=file)
            last_modified = response["LastModified"]
            file_info.append((file, last_modified))

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Ñ–∞–π–ª—ã –ø–æ –¥–∞—Ç–µ (—Å—Ç–∞—Ä—ã–µ ‚Üí –Ω–æ–≤—ã–µ)
        file_info.sort(key=lambda x: x[1])

        # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä–µ–π—à–∏–µ —Ñ–∞–π–ª—ã (–æ—Å—Ç–∞–≤–ª—è–µ–º max_files)
        files_to_delete = file_info[:len(filtered_files) - max_files]
        for file, _ in files_to_delete:
            s3.delete_object(Bucket="boyarinnbotbucket", Key=file)
            logger.info(f"üóë –£–¥–∞–ª—ë–Ω —Å—Ç–∞—Ä—ã–π –∞—Ä—Ö–∏–≤–Ω—ã–π —Ñ–∞–π–ª: {file}")

        logger.info(f"‚úÖ –û—á–∏—Å—Ç–∫–∞ –∞—Ä—Ö–∏–≤–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –û—Å—Ç–∞–≤–ª–µ–Ω–æ {max_files} —Ñ–∞–π–ª–æ–≤.")

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—á–∏—Å—Ç–∫–µ –∞—Ä—Ö–∏–≤–∞: {e}")


def list_files_in_folder(s3, folder_prefix):
    try:
        response = s3.list_objects_v2(Bucket=B2_BUCKET_NAME, Prefix=folder_prefix)
        return [
            obj['Key'] for obj in response.get('Contents', [])
            if obj['Key'] != folder_prefix and not obj['Key'].endswith('.bzEmpty') and FILE_NAME_PATTERN.match(os.path.basename(obj['Key']))
        ]
    except ClientError as e:
        logger.error(f"Error listing files in {folder_prefix}: {e.response['Error']['Message']}")
        return []

def get_ready_groups(files):
    groups = {}
    for file_key in files:
        base_name = os.path.basename(file_key)
        if FILE_NAME_PATTERN.match(base_name):
            group_id = base_name.rsplit('.', 1)[0]
            groups.setdefault(group_id, []).append(base_name)

    ready_groups = []
    for group_id, file_list in groups.items():
        expected_files = [group_id + ext for ext in FILE_EXTENSIONS]
        if all(file in file_list for file in expected_files):
            ready_groups.append(group_id)

    return ready_groups

def handle_publish(s3, config_data):
    publish_folder = config_data.get("publish")
    if not publish_folder:
        return

    files = list_files_in_folder(s3, publish_folder)
    if not files:
        return

    for file_key in files:
        archive_key = file_key.replace(publish_folder, ARCHIVE_FOLDER)
        try:
            s3.copy_object(Bucket=B2_BUCKET_NAME, CopySource={"Bucket": B2_BUCKET_NAME, "Key": file_key}, Key=archive_key)
            s3.delete_object(Bucket=B2_BUCKET_NAME, Key=file_key)
        except ClientError as e:
            logger.error(f"Error archiving {file_key}: {e.response['Error']['Message']}")

    config_data.pop("publish", None)
    save_config_public(s3, config_data)

def move_group(s3, src_folder, dst_folder, group_id):
    for ext in FILE_EXTENSIONS:
        src_key = f"{src_folder}{group_id}{ext}"
        dst_key = f"{dst_folder}{group_id}{ext}"
        try:
            s3.head_object(Bucket=B2_BUCKET_NAME, Key=src_key)
            s3.copy_object(Bucket=B2_BUCKET_NAME, CopySource={"Bucket": B2_BUCKET_NAME, "Key": src_key}, Key=dst_key)
            s3.delete_object(Bucket=B2_BUCKET_NAME, Key=src_key)
        except ClientError as e:
            if e.response['Error']['Code'] != "NoSuchKey":
                logger.error(f"Error moving {src_key}: {e.response['Error']['Message']}")

def process_folders(s3, folders):
    """–ü–µ—Ä–µ–º–µ—â–∞–µ—Ç —Ñ–∞–π–ª—ã –º–µ–∂–¥—É 666/ ‚Üí 555/ ‚Üí 444/ (–æ—Ç –±–æ–ª—å—à–µ–π –ø–∞–ø–∫–∏ –∫ –º–µ–Ω—å—à–µ–π)."""
    empty_folders = set()
    changes_made = True

    while changes_made:
        changes_made = False
        for i in range(len(folders) - 1):  # –ò–∑–º–µ–Ω–µ–Ω–æ: —Ç–µ–ø–µ—Ä—å –∏–¥—ë–º –æ—Ç 666 –∫ 444
            src_folder = folders[i]
            dst_folder = folders[i + 1]

            if src_folder in empty_folders:
                continue

            src_files = list_files_in_folder(s3, src_folder)
            dst_files = list_files_in_folder(s3, dst_folder)

            logger.info(f"üìÇ –ü—Ä–æ–≤–µ—Ä—è–µ–º {src_folder} ‚Üí {dst_folder}")
            logger.info(f"–§–∞–π–ª—ã –≤ {src_folder}: {src_files}")
            logger.info(f"–§–∞–π–ª—ã –≤ {dst_folder}: {dst_files}")

            src_ready = get_ready_groups(src_files)
            dst_ready = get_ready_groups(dst_files)

            for group_id in src_ready:
                logger.info(f"üì¶ –ü–µ—Ä–µ–º–µ—â–∞–µ–º –≥—Ä—É–ø–ø—É {group_id} –∏–∑ {src_folder} –≤ {dst_folder}")
                move_group(s3, src_folder, dst_folder, group_id)
                changes_made = True

            if not src_ready:
                empty_folders.add(src_folder)

    return list(empty_folders)

def main():
    """–û—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–æ—Ü–µ—Å—Å B2 Storage Manager."""
    logger.info("üîÑ –ó–∞–ø—É—Å–∫ B2 Storage Manager...")

    try:
        s3 = get_b2_client()  # –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ B2

        # 1Ô∏è‚É£ –ó–∞–≥—Ä—É–∂–∞–µ–º config_public.json –∏ –ø–æ–ª—É—á–∞–µ–º –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—ã–µ generation_id
        config_public, published_generations = load_config_public(s3)

        # 2Ô∏è‚É£ –ü–µ—Ä–µ–º–µ—â–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤ –º–µ–∂–¥—É –ø–∞–ø–∫–∞–º–∏ (666 ‚Üí 555 ‚Üí 444)
        logger.info("üìÇ –ü–µ—Ä–µ–º–µ—â–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤ –º–µ–∂–¥—É –ø–∞–ø–∫–∞–º–∏...")
        process_folders(s3, ["666/", "555/", "444/"])

        # 3Ô∏è‚É£ –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Ñ–∞–π–ª—ã –≤ 444/ (–≥–æ—Ç–æ–≤—ã–µ –∫ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏)
        files_to_publish = list_files_in_folder(s3, "444/")

        if files_to_publish:
            # 4Ô∏è‚É£ –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –Ω–æ–≤—ã–π generation_id
            generation_id = generate_file_id().replace(".json", "")  # –£–±–∏—Ä–∞–µ–º .json –∏–∑ –∏–º–µ–Ω–∏
            logger.info(f"üìÑ –ü—É–±–ª–∏–∫–∞—Ü–∏—è –≥—Ä—É–ø–ø—ã —Å generation_id: {generation_id}")

            # 5Ô∏è‚É£ –ó–∞–ø–∏—Å—ã–≤–∞–µ–º generation_id –≤ config_public.json
            handle_publish(s3, config_public, generation_id)

        # 6Ô∏è‚É£ –ê—Ä—Ö–∏–≤–∞—Ü–∏—è –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –ø–æ generation_id
        for gen_id in published_generations:
            logger.info(f"üîç –ê—Ä—Ö–∏–≤–∏—Ä—É–µ–º generation_id {gen_id}, –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã: {files}")
            files = list_files_by_generation_id(s3, gen_id)
            if files:
                archive_files(s3, files)
            else:
                logger.info(f"‚ö†Ô∏è –ù–µ—Ç —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∞—Ä—Ö–∏–≤–∞—Ü–∏–∏ –ø–æ generation_id {gen_id}")

        # 7Ô∏è‚É£ –û—á–∏—Å—Ç–∫–∞ –∞—Ä—Ö–∏–≤–∞ (—É–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä—ã—Ö —Ñ–∞–π–ª–æ–≤)
        cleanup_archive(s3)

        # 8Ô∏è‚É£ –ó–∞–ø—É—Å–∫–∞–µ–º generate_content.py, –µ—Å–ª–∏ 666/ –ø—É—Å—Ç–∞—è
        files_in_666 = list_files_in_folder(s3, "666/")  # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤

        if not files_in_666:  # –ï—Å–ª–∏ –ø–∞–ø–∫–∞ –ø—É—Å—Ç–∞
            logger.info("‚ö†Ô∏è –ü–∞–ø–∫–∞ 666/ –ø—É—Å—Ç–∞—è. –ó–∞–ø—É—Å–∫–∞–µ–º generate_content.py...")
            try:
                subprocess.run(["python", os.path.join(config.get('FILE_PATHS.scripts_folder'), "generate_content.py")],
                               check=True)
                logger.info("‚úÖ –°–∫—Ä–∏–ø—Ç generate_content.py –≤—ã–ø–æ–ª–Ω–µ–Ω —É—Å–ø–µ—à–Ω–æ.")
            except subprocess.CalledProcessError as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ generate_content.py: {e}")
        else:
            logger.info(f"üìÇ –í 666/ –æ—Å—Ç–∞–ª–∏—Å—å —Ñ–∞–π–ª—ã: {files_in_666}")

    except Exception as e:
        handle_error(logger, e, "‚ùå –û—à–∏–±–∫–∞ –≤ B2 Storage Manager")

if __name__ == "__main__":
    main()
