import os
import json
import boto3
import logging
import re

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

# –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
B2_ACCESS_KEY = os.getenv("B2_ACCESS_KEY")
B2_SECRET_KEY = os.getenv("B2_SECRET_KEY")
B2_BUCKET_NAME = os.getenv("B2_BUCKET_NAME")
B2_ENDPOINT = os.getenv("B2_ENDPOINT")

REQUIRED_SUFFIXES = [".json", ".png", ".mp4", "_sarcasm.png"]
FOLDERS = ["444/", "555/", "666/"]
CONFIG_PUBLIC_PATH = "config/config_public.json"
GEN_ID_REGEX = re.compile(r"^(\d{8}-\d{4})")

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–ª—é—á–µ–π
if not all([B2_ACCESS_KEY, B2_SECRET_KEY, B2_BUCKET_NAME, B2_ENDPOINT]):
    logging.error("‚ùå –ù–µ –≤—Å–µ –∫–ª—é—á–∏ B2 –∑–∞–¥–∞–Ω—ã –≤ –æ–∫—Ä—É–∂–µ–Ω–∏–∏!")
    exit(1)

# –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ B2
s3 = boto3.client(
    "s3",
    endpoint_url=B2_ENDPOINT,
    aws_access_key_id=B2_ACCESS_KEY,
    aws_secret_access_key=B2_SECRET_KEY
)

def clear_published_ids():
    try:
        obj = s3.get_object(Bucket=B2_BUCKET_NAME, Key=CONFIG_PUBLIC_PATH)
        content = json.loads(obj['Body'].read().decode('utf-8'))

        if 'generation_id' in content:
            logging.info(f"üßπ –û—á–∏—â–∞—é {len(content['generation_id'])} ID –∏–∑ config_public.json...")
            content['generation_id'] = []
            s3.put_object(
                Bucket=B2_BUCKET_NAME,
                Key=CONFIG_PUBLIC_PATH,
                Body=json.dumps(content, ensure_ascii=False, indent=4).encode('utf-8')
            )
            logging.info("‚úÖ config_public.json –æ–±–Ω–æ–≤–ª—ë–Ω.")
        else:
            logging.info("‚ÑπÔ∏è –ö–ª—é—á 'generation_id' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ config_public.json.")
    except Exception as e:
        logging.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—á–∏—Å—Ç–∫–µ config_public.json: {e}")

def list_all_objects(prefix):
    all_keys = []
    paginator = s3.get_paginator('list_objects_v2')
    for page in paginator.paginate(Bucket=B2_BUCKET_NAME, Prefix=prefix):
        for obj in page.get("Contents", []):
            all_keys.append(obj["Key"])
    return all_keys

def delete_incomplete_groups():
    for folder in FOLDERS:
        logging.info(f"üîç –ü—Ä–æ–≤–µ—Ä—è—é {folder} –Ω–∞ –Ω–µ–ø–æ–ª–Ω—ã–µ –≥—Ä—É–ø–ø—ã...")
        keys = list_all_objects(folder)

        group_map = {}

        for key in keys:
            filename = os.path.basename(key)
            matched = None

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –æ–±—ã—á–Ω—ã–µ —Å—É—Ñ—Ñ–∏–∫—Å—ã
            for suffix in REQUIRED_SUFFIXES:
                if filename.endswith(suffix):
                    matched = suffix
                    break

            if matched:
                if matched == "_sarcasm.png":
                    base = filename[:-len("_sarcasm.png")]
                else:
                    base = filename.replace(matched, "")
                if GEN_ID_REGEX.match(base):
                    group_map.setdefault(base, set()).add(matched)

        for gen_id, found_suffixes in group_map.items():
            if set(found_suffixes) != set(REQUIRED_SUFFIXES):
                # –£–¥–∞–ª—è–µ–º –≤—Å–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —á–∞—Å—Ç–∏
                for suffix in found_suffixes:
                    key_to_delete = f"{folder}{gen_id}{suffix}"
                    try:
                        s3.delete_object(Bucket=B2_BUCKET_NAME, Key=key_to_delete)
                        logging.info(f"üóëÔ∏è –£–¥–∞–ª—ë–Ω: {key_to_delete}")
                    except Exception as e:
                        logging.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å {key_to_delete}: {e}")

if __name__ == "__main__":
    clear_published_ids()
    delete_incomplete_groups()
